<!doctype html>
<html lang="en">
  <head>
    <title>utf9k</title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="theme-color" content="#eaebe3" media="(prefers-color-scheme: light)" />
    <meta name="theme-color" content="#1f2738" media="(prefers-color-scheme: dark)" />
    <meta name="author" content="Marcus Crane" />
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no" />
    <link rel="shortcut icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" href="/favicon.png" sizes="16x16" />
    <link rel="apple-touch-icon" href="/favicon.png" />
    <link rel="me" href="mailto:hello@utf9k.net" />
    <link rel="me" href="https://mastodon.nz/@utf9k" />
    <link rel="me" href="https://twitter.com/sentreh" />
    <link rel="me" href="https://github.com/marcus-crane" />
    <link rel="webmention" href="https://webmention.io/utf9k.net/webmention" />
    <link rel="pingback" href="https://webmention.io/utf9k.net/xmlrpc" />
    <link rel="stylesheet" href="https://unpkg.com/littlefoot@4.0.1/dist/littlefoot.css" />
    <link rel="stylesheet" href="/css/style.css" />
  </head>
  <body>
    <header class="navigation">
      <a href="/" id="logo">
        <img src="/favicon.png" alt="utf9k" width="16px" height="16px" />
        <span>utf9k</span>
      </a>
      <nav>
        <li><a href="/blog/">Blog</a></li>
        <span>//</span>
        <li><a href="/projects/">Projects</a></li>
        <span>//</span>
        <li><a href="/about/">About</a></li>
        <span>//</span>
        <li><a href="/questions/">Questions</a></li>
        <span>//</span>
        <li><a href="/rss.xml">RSS</a></li>
      </nav>
    </header>
    <div id="content">
      <article>
        <header>
          <h1>Sampling all the way down</h1>

          <div id="post-meta">
            <time datetime="Mon Aug 08 2022 18:00:00 GMT+1200 (New Zealand Standard Time)">August 8, 2022</time> ·
            Around 8 minutes
          </div>
        </header>
        <div id="post-body">
          <h2 id="table-of-contents">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#table-of-contents">¶ </a>Table of Contents
          </h2>
          <ul>
            <li><a href="#preamble">Preamble</a></li>
            <li><a href="#the-outer-wall">The outer wall</a></li>
            <li><a href="#application-sampling">Application sampling</a></li>
            <li><a href="#agent-sampling">Agent sampling</a></li>
            <li><a href="#ingestion-sampling">Ingestion Sampling</a></li>
            <li><a href="#retention-filters">Retention Filters</a></li>
            <li><a href="#the-secret-hidden-boss">The Secret Hidden Boss</a></li>
            <li><a href="#it-doesnt-matter-in-the-end">It doesn't matter in the end</a></li>
          </ul>
          <h2 id="preamble"><a aria-hidden="true" class="jumplink" tabindex="-1" href="#preamble">¶ </a>Preamble</h2>
          <p>
            Recently, I was chatting with a coworker about the accuracy of tracing. In particular, we were discussing
            <a href="https://docs.datadoghq.com/tracing/">Datadog APM</a> but the same discussion probably applies
            equally to <a href="https://opentelemetry.io/">OpenTelemetry</a> as well as other SaaS provider
            implementations.
          </p>
          <p>
            To be clear, when I say accuracy here, I don't mean the accuracy of what each trace is presenting. The
            overall picture is more than accurate enough but I would argue that it will never 100% reflect reality, nor
            is that necessarily something valuable to obtain.
          </p>
          <p>
            I thought it might be interesting, both for my own future reference but also just as a general discussion to
            go through some of the various ways that tracing may not reflect reality in its entirety.
          </p>
          <p>
            <center>
              <figure>
                <img
                  src="https://datadog-docs.imgix.net/images/tracing/apm_lifecycle/apm_lifecycle_0.c5017c20b222d73a29b5a1a223f83eb4.png?auto=format"
                  alt="A diagram showing various parts of the Datadog ingestion pipeline. The screenshot is just meant to reflect the sprawl of different bits involved in the pipeline. The actual elements of the pipeline itself aren&#x27;t so important for this image."
                />
              </figure>
            </center>
          </p>
          <h2 id="the-outer-wall">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#the-outer-wall">¶ </a>The outer wall
          </h2>
          <p>
            First of all, there are surely some portion of requests that will never reach your servers in the first
            place.
          </p>
          <p>
            There are probably not a lot in accurate but you might imagine various scenarios like someone attempting to
            visit your website only for their ISP's DNS server to throw a fit, or someone browsing on a train only for
            their connection to get temporarily blackholed as they pass through a concrete tunnel.
          </p>
          <p>These requests won't even reach you so they won't even show up in your tracing.</p>
          <h2 id="application-sampling">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#application-sampling">¶ </a>Application sampling
          </h2>
          <p>
            <center>
              <figure>
                <img
                  src="https://cdn.utf9k.net/blog/sampling-all-the-way-down/apps.jpg"
                  alt="A version of the Datadog ingestion pipeline, with a section titled Your instrumented apps being selected. It reflects the part of the pipeline we are going to discuss in this section."
                />
              </figure>
            </center>
          </p>
          <p>
            So we've got requests hitting our application so we capture 100% of those requests as traces and life is
            easy right?
          </p>
          <p>If only it were so easy.</p>
          <p>
            By default, your service will save
            <a
              href="https://docs.datadoghq.com/tracing/trace_pipeline/ingestion_mechanisms/?tab=go#in-tracing-libraries-user-defined-rules"
              >100 traces per second</a
            >
            unless you explicitly set an override.
          </p>
          <p>
            Datadog does recommend setting a sample rate of 100%, as you're only charged for traces indexed and retained
            beyond the initial 15 minute ingestion window.
          </p>
          <p>
            You may also have cases where code within your application could be
            <a href="https://docs.datadoghq.com/tracing/trace_pipeline/ingestion_mechanisms/?tab=go#force-keep-and-drop"
              >explicitly causing traces to be discarded</a
            >
            if they're not useful. The inverse is true with retaining specific traces as well.
          </p>
          <p>
            The above also assumes that your application is configured properly as well to ensure those traces aren't
            being emitted into a black hole.
          </p>
          <h2 id="agent-sampling">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#agent-sampling">¶ </a>Agent sampling
          </h2>
          <p>
            <center>
              <figure>
                <img
                  src="https://cdn.utf9k.net/blog/sampling-all-the-way-down/apps.jpg"
                  alt="The same Datadog ingestion pipeline shown in the previous image with no changes."
                />
              </figure>
            </center>
          </p>
          <p>Now we go a level high to the process that receives all of your traces.</p>
          <p>
            It has its
            <a href="https://docs.datadoghq.com/tracing/trace_pipeline/ingestion_mechanisms/?tab=go#in-the-agent"
              >own sampling</a
            >
            which will only retain ~10 traces/second per agent instance. It's worth noting that this is overridden by
            setting your application sample rate explicitly but if you haven't, you'll be seeing a portion of reality.
          </p>
          <p>
            You might also get rate limited, causing traces to probably be discarded, if you have
            <a href="https://github.com/DataDog/datadog-agent/blob/main/pkg/config/config_template.yaml#L1152"
              >too many services connecting to a single agent</a
            >,
            <a href="https://github.com/DataDog/datadog-agent/blob/main/pkg/config/config_template.yaml#L1082"
              >your agent is using too much memory</a
            >
            or
            <a href="https://github.com/DataDog/datadog-agent/blob/main/pkg/config/config_template.yaml#L1091"
              >your agent is using too much CPU</a
            >.
          </p>
          <p>
            <center>
              <figure>
                <img
                  src="https://datadog-docs.imgix.net/images/tracing/trace_indexing_and_ingestion/service_traffic_breakdown.a0e421f3503064b7d03f1c3683a67fc4.png?auto=format"
                  alt="A screenshot of the Datadog service ingestion screen showing a service that has <0.1% of traces sampled due to agent CPU or RAM usage"
                />
              </figure>
            </center>
          </p>
          <p>
            On top of all this, there's further sampling beyond the "10 traces/second" which is a
            <a
              href="https://docs.datadoghq.com/tracing/troubleshooting/agent_rate_limits/#maximum-events-per-second-limit"
              >max of 200 trace events per second</a
            >. What is a trace event versus a trace you ask? I have no idea!
          </p>
          <p>
            You should probably also disable the
            <a href="https://github.com/DataDog/datadog-agent/blob/main/pkg/config/config_template.yaml#L1068"
              >error sampler</a
            >
            so it doesn't impact your picture of reality too for maximum accuracy.
          </p>
          <p>
            Once again, this too assumes that all of your agent instances are properly configured and aren't dropping
            traces off a cliff or else you'll be missing a portion of reality.
          </p>
          <h2 id="ingestion-sampling">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#ingestion-sampling">¶ </a>Ingestion Sampling
          </h2>
          <p>
            <center>
              <figure>
                <img
                  src="https://datadog-docs.imgix.net/images/tracing/apm_lifecycle/ingestion_sampling_rules.664739c275df2d563b782552d467422e.png?auto=format"
                  alt="A version of the Datadog ingestion pipeline, with a section titled Ingestion sampling rules being selected. It reflects the part of the pipeline we are going to discuss in this section."
                />
              </figure>
            </center>
          </p>
          <p>Oh, you thought we were done? We're just getting started because we've got more things to toggle.</p>
          <p>
            So you've configured your application and agent perfectly but you're still not perfectly reflecting reality.
            <a href="https://docs.datadoghq.com/tracing/trace_pipeline/ingestion_controls/#service-ingestion-summary"
              >Did you know upstream services can decide whether traces involving your service are discarded or
              retained?</a
            >.
          </p>
          <p>
            <center>
              <figure>
                <img
                  src="https://datadog-docs.imgix.net/images/tracing/trace_indexing_and_ingestion/service_ingestion_summary.fd43c5c59b2c9273cd84e8467b958194.png?auto=format"
                  alt="A screenshot of the Service Ingestion Summary screen showing a service. Visible to the right side are all of the upstream services that have influence on the displayed services trace sampling decisions."
                />
              </figure>
            </center>
          </p>
          <p>
            You should probably go talk to your coworkers who maintain upstream services and argue the case for never
            sampling a single trace. Depending on who controls the credit card, it may not be quite as compelling when
            considering the huge bill that may be involved.
          </p>
          <h2 id="retention-filters">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#retention-filters">¶ </a>Retention Filters
          </h2>
          <p>
            <center>
              <figure>
                <img
                  src="https://datadog-docs.imgix.net/images/tracing/apm_lifecycle/retention_filters.5e6f2b4eedbe790caf41a7e090048deb.png?auto=format"
                  alt="A version of the Datadog ingestion pipeline, with a section titled Retention filters being selected. It reflects the part of the pipeline we are going to discuss in this section."
                />
              </figure>
            </center>
          </p>
          <p>Ok, we're nearly through the gauntlet because all we've got left are retention filters.</p>
          <p>
            So, let's say that your application sends 100% of traces to Datadog and never gets sampled along the way.
            That just means your traces are 100% visible for a mere 15 minutes before more sampling kicks in.
          </p>
          <p>
            Retention filters are the final boss here, where by default, Datadog maintains all errors (across a diverse
            range of errors codes) and a handful of successful traces based on their latencies, status codes and so on.
          </p>
          <p>
            Now you could simply mark all traces as being retained beyond that 15 minute window but I hope you're ready
            to re-mortgage your house to afford it because indexing ain't cheap, beyond the defaults chosen by Datadog
            themselves.
          </p>
          <p>
            You also need to ensure that no one else has set up any retention filters that might sample your application
            in any way so fire up those access control policies.
          </p>
          <p>
            Assuming you're a rich lister however, you've made it to the end... right? All is right with the world and
            we have a perfectly perfect view of the world.
          </p>
          <h2 id="the-secret-hidden-boss">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#the-secret-hidden-boss">¶ </a>The Secret Hidden
            Boss
          </h2>
          <p>
            Even if you have every single trace ever generated, perfection is still out of reach because the insights
            into our traces themselves may be skewed, short of our combing through all of the data ourselves which is
            likely not possible at scale.
          </p>
          <p>
            To understand more, we need to briefly talk about
            <a href="https://arxiv.org/pdf/1908.10693.pdf">DDSketch</a> which is Datadog's "sketching" algorithm.
          </p>
          <p>
            I hadn't heard of the term before but just as a sketch in drawing is an approximation of a fleshed out
            drawing, a sketch in computer science is an approximation of some set of data.
          </p>
          <p>
            The sketch is not a full accurate set of all existing data points but as an approximation, we can infer
            things that are roughly true about the source data.
          </p>
          <p>
            For example, let's say our true data set contains 1 million values and the data set is effectively a random
            sampling.
          </p>
          <p>
            We can take a sample of 1000 values from our source data set and in most cases, the median value within our
            sample is likely to be about the same as it is in our source data set.
          </p>
          <p>
            While it may not be 100% precise, depending on our use case, it's probably good enough without the overhead
            of having to process the full dataset.
          </p>
          <p>Where this doesn't work however is with quantiles, or percentiles.</p>
          <p>
            Let's say we wanted to find the maximum value in our 1 million data points. You could just iterate through
            all of them and keep track of the highest value you've seen. There's no need to keep every data point in
            memory, just the current value you're looking at and the highest value you've seen so far.
          </p>
          <p>
            What about finding the 99% percentile? Well, we need to retain all of the values we've seen so far in order
            to continually compare values we're seeing against our 1% grouping and further comparing that to our full
            dataset to ensure that we're adding and removing the right values.
          </p>
          <p>
            Such a thing would get quite expensive which is where sketching comes in and it's what Datadog does to your
            data.
          </p>
          <p>
            Rather than perfectly calculating percentiles down to the very last decimal place, traces can be "bucketed"
            into rounded that are near enough to capture what they represent but we only need to store the bucket values
            and not every single value that is allocated to a bucket.
          </p>
          <p>
            <center>
              <figure>
                <img
                  src="https://imgix.datadoghq.com/img/blog/engineering/computing-accurate-percentiles-with-ddsketch/ddsketch_diagram_6_190911.png?auto=format&#x26;fit=max&#x26;w=847&#x26;dpr=2"
                  alt="A screenshot showing latency buckets. There are three labelled 100 milliseconds, 102 milliseconds and 104 milliseconds. Traces that a few decimal places outside of each bucket are sampled into their closest buckets. For example, a trace that took 104.4 milliseconds is sampled into the 104 millisecond bucket."
                />
              </figure>
            </center>
          </p>
          <p>
            That is effectively the sketch in question and you can read more about the entire process
            <a href="https://www.datadoghq.com/blog/engineering/computing-accurate-percentiles-with-ddsketch/"
              >in this blog post</a
            >.
          </p>
          <p>
            Suffice to say, even if you did manage to capture all traces, the insights you're getting from them have
            likely been skewed from reality ever so slightly through the sketching process so you'll never win unless
            you handle each trace yourself.
          </p>
          <h2 id="it-doesnt-matter-in-the-end">
            <a aria-hidden="true" class="jumplink" tabindex="-1" href="#it-doesnt-matter-in-the-end">¶ </a>It doesn't
            matter in the end
          </h2>
          <p>
            More than anything, this post is partly a technical guide but mostly fun exercise with the end of goal of
            saying "Who cares".
          </p>
          <p>
            I'm a big fan of the short story
            <a href="https://en.wikipedia.org/wiki/On_Exactitude_in_Science">On Exactitude in Science</a>. It's a short
            story about a kingdom who become so good at map making that they make the most accurate map.
          </p>
          <p>
            The problem is that it's 1:1 scale with the kingdom itself in order to capture it in perfect detail. Such a
            map may be precise but it's so detailed and unwieldy as to be useless.
          </p>
          <p>I'm a big advocate of the same idea in software.</p>
          <p>Partly so I can be lazy but also because beyond a certain point, true accuracy doesn't really matter.</p>
          <p>
            What you want is just enough detail to know that things are happening but further detail is just diminishing
            returns.
          </p>
          <p>Let's say that you want to reflect rate limits for a particular customer.</p>
          <p>Does it really matter whether they're being rate limited 5 times per day or 1000 times per day?</p>
          <p>
            Beyond a certain point, the damage is already done and they're having a bad time so knowing the true extent
            doesn't really change the facts of the situation.
          </p>
          <p>
            It can be interesting to know for sure and you might want to know but there is probably no such thing as a
            truly accurate value, whether it's tracing, logs or something else.
          </p>
          <p>
            You're probably better off just getting something good enough and then having a nap or investing that time
            into other improvements.
          </p>
        </div>
      </article>

      <script src="https://unpkg.com/littlefoot@4.0.1/dist/littlefoot.js" type="text/javascript"></script>
      <script type="text/javascript">
        littlefoot.littlefoot(); // Pass any littlefoot settings here.
      </script>
    </div>
    <div id="fullscreen"></div>
    <script type="module">
      import "/js/image-viewer.js";
    </script>
  </body>
</html>
