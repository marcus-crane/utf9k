[{"content":"No! As I learned recently, only a handful of regions feature multiple availability zones.\nHaving originally started out using AWS, I had assumed that every region has multiple availability zones.\nAzure maintains a list of regions with multiple AZs so if you need redundancy, you’re best off picking one of these.\nSome services may refuse to deploy entirely (such as “geo-redundant” gateways in us-east as I found out recently)\n","slug":"azure-regions-alike","tags":["availability","azure","cloud","microsoft"],"title":"Are all Azure regions alike?"},{"content":"I’ve known about this tool for some time now but I’m writing about it because I ALWAYS forget to use it.\npbcopy, and as I just discovered, pbpaste are two tools that are built into macOS.\nYou can pipe data into the former to add it to your clipboard and similarly, you can use the latter as input into a unix pipeline.\nLet’s look at an example:\n\u003e echo \"see you on the other side\" | pbcopy You can now use Ctrl+V to paste this text into any GUI application. Saves you having to mouse over to the terminal and highlight text but I still do it every darn time.\nWe can also use our clipboard contents too as mentioned. You could have copied some text from a GUI application and you want to use it in your terminal.\n# Clipboard contains \"utf9k.net\" that I copied from my browser \u003e pbpaste | xargs dig TXT | grep \"I see\" utf9k.net. 3444 IN TXT \"I see you snoopin' around ;) If you're after something, you can feel fr\\010ee to email me at marcus@utf9k.net\" What I’m trying to say is that I have all of the tools at my disposal to avoid RSI but I just need to remember they exist…\n","slug":"macos-clipboard-piping","tags":["clipboard","macos","terminal"],"title":"How can I access my clipboard contents inside my terminal?"},{"content":"The following should roughly do it. Your mileage may vary!\ngit clone -b emacs-27 git://git.sv.gnu.org/emacs.git cd emacs sudo apt-get build-dep emacs ./autogen.sh ./configure --with-x-toolkit=lucid --with-mailutils make -j4 ./src/emacs // test that its working sudo make install ","slug":"emacs-compile-from-source","tags":["emacs"],"title":"How can I compile Emacs from source?"},{"content":"Recently, a coworker of mine got a new laptop and needed to connect to the printer at work. One of the dialog boxes asked for the “print queue”.\nFor the unfamiliar, here’s what the macOS printer settings look like.\nI can’t see any queue settings so let’s dive a little deeper.\nNothing here either but surely there must be something under the hood. Thankfully, there’s a built in command called lpstat that allows all sorts of printer configuration.\n\u003e man lpstat | grep lpstat lpstat(1) Apple Inc. lpstat(1) lpstat - print cups status information 26 April 2019 CUPS lpstat(1) In order to find the printer queue name, I was able to make use of lpstat -s like so:\n\u003e lpstat -s system default destination: example_printer device for example_printer: ipp://example-printer/my-fake-queue Ah, so the queue name is my-fake-queue. I wish the System Preferences pane had just said so earlier.\nWhile there, I also discovered a bunch of my old print jobs as well!\n\u003e lpstat -W completed -l example_printer-3 marcus 59392 Wed 28 Apr 09:40:30 2021 Status: The printer is not responding. Alerts: processing-to-stop-point queued for example_printer example_printer-2 marcus 113664 Wed 17 Mar 15:36:56 2021 Status: The printer is unreachable at this time. Alerts: job-canceled-by-user queued for example_printer example_printer-1 marcus 51200 Thu 8 Oct 11:14:01 2020 Status: Alerts: processing-to-stop-point queued for example_printer Hopefully this makes your printing life easier, or at least gives you some closure on why those months old jobs refused to print.\n","slug":"macos-printer-cli","tags":["macos","printers"],"title":"How can I configure my printer via terminal on macOS?"},{"content":"If you’re trying to test out a job, and don’t want to wait for however long, you can manually create a job instance.\nAssuming our cronjob is called sports-leaderboard-calc, you can create it like so:\n\u003e kubectl create job instance-name --from=cronjob/sports-leaderboard-calc job.batch/instance-name You’ll then see the resulting job and pod under kubectl get job and kubectl get pod respectively.\n","slug":"kubes-create-cron-instance","tags":["cronjob","kubernetes"],"title":"How can I create an instance of a Kube cronjob?"},{"content":"I suppose they aren’t used too much anymore but I’ve started using them as a preview window for my projects page.\nIt can be handy to act different depending on an iFrame, such as scaling the view port.\nYou can’t do something like this:\niframe \u003e canvas { width: 500px; } canvas { width: 100%; } but you can use Javascript inside an iFrame and make the changes within the frame itself, rather than from the outside:\nfunction insideIframe() { try { return window.self !== window.top } catch (e) { return true } } if (insideIframe()) { // perhaps change the size of something or just act differently } ","slug":"js-detect-iframe-parent","tags":["iframe","javascript"],"title":"How can I determine if my code is inside of an iFrame?"},{"content":"This question was trending on Hacker News but the thread in question never addressed it.\nBuried down in the comments was a technical fix suggested by torstenvl.\nSafari has a few configuration entries accessible via defaults read com.apple.coreservices.uiagent.\nWhile I haven’t tested this personally, torstenvl recommended stubbing out the notification with the following commands:\ndefaults write com.apple.coreservices.uiagent CSUIHasSafariBeenLaunched -bool YES defaults write com.apple.coreservices.uiagent CSUIRecommendSafariNextNotificationDate -date 2050-01-01T00:00:00Z defaults write com.apple.coreservices.uiagent CSUILastOSVersionWhereSafariRecommendationWasMade -float 99.99 If this works for you, let me know. I’m currently running the macOS Monterey beta at the time of writing and as I’ve already used Safari, I don’t believe I get this notification anymore.\n","slug":"macos-disable-safari-recommendation","tags":["macos","safari","software"],"title":"How can I disable the 'Try the new Safari' notification?"},{"content":"Using pg_dump, which ships with the psql executable, it’s a pretty simple progress\npg_dump --dbname={{DBNAME}} --host={{HOST}} --port={{PORT}} --username={{USERNAME}} --password --format=c \u003e {{NAME}}.dump # The c in --format=c stands for custom ","slug":"postgres-export-db","tags":["databases","postgres"],"title":"How can I export a Postgres database?"},{"content":"This error is usually pretty cryptic and I often forget how to debug it so let’s look at a sample error:\nError: The module '/Users/marcus/Code/octowise/node_modules/better-sqlite3/build/Release/better_sqlite3.node' was compiled against a different Node.js version using NODE_MODULE_VERSION 83. This version of Node.js requires NODE_MODULE_VERSION 89. I often remember that I need to possibly use a different version of nodejs but I never remember how to tell which one.\nThe official NodeJS site has a table with version numbers and their corresponding NODE_MODULE_VERSION available here.\nIn the case of this error, I think I probably want to downgrade to Node.js 14.x? It’s all very confusing.\n","slug":"nodejs-module-version","tags":["javascript","nodejs"],"title":"How can I find my current NODE_MODULE_VERSION?"},{"content":" While there’s the classic Apple menu -\u003e About This Mac -\u003e System Report, a terminal based alternative is the system_profiler command.\nYou can use a list of queryable types like so:\n\u003e system_profiler -listDataTypes Available Datatypes: SPParallelATADataType SPUniversalAccessDataType [...] Once you’ve found one or more types, you’re interested in then just append it after the command like so: system_profiler \u003ctype1\u003e \u003ctype2\u003e\nLet’s see how it looks in action:\n\u003e system_profiler SPPowerDataType Power: Battery Information: Model Information: Manufacturer: DSY Device Name: bq20z451 Pack Lot Code: 0 PCB Lot Code: 0 Firmware Version: 1002 Hardware Revision: 1 Cell Revision: 2400 Charge Information: Fully Charged: No Charging: Yes Full Charge Capacity (mAh): 4569 State of Charge (%): 74 Health Information: Cycle Count: 81 Condition: Normal This is just an excerpt of what is otherwise a whole bunch of information.\nParticularly interesting is the SPAirPortDataType which can be queried to see a list of SSIDs in the environment.\n","slug":"macos-view-hardware","tags":["hardware","macos"],"title":"How can I find out more about the hardware inside my Mac?"},{"content":"You can see a list of current auth-sources by running the following elisp function\n\u003e auth-sources (password-store \"~/.authinfo.gpg\") ","slug":"emacs-auth-sources","tags":["elisp","emacs"],"title":"How can I find out where Emacs is checking for passwords?"},{"content":"This is arguably one of the more obscure commands I’ve come across. At the time, a coworker of mine was having issues where his laptop would restart seemingly at random.\nWe were able to find out a bit more with the following command:\nlog show -predicate 'eventMessage contains \"Previous shutdown cause\"' -last 24h It may take a minute or so to actually find some logs but it should reveal a shutdown code.\nI don’t remember where I dug it up but you can see a list of shutdown causes and their meanings in this PDF.\nHere’s how the results looks on my machine where I had performed a normal shutdown as a test\nIf we compare the shutdown code to the PDF above, we can see the description is Correct shut down which lines up exactly.\nNow let’s take this information and use it to see what was potentially happening to my coworkers laptop.\nHere’s a screenshot of his terminal window with the same command:\nGoing back to the PDF again, we can see that -128 is an alias for -112. Checking -112 tells us that it is “Probably memory related” which at least narrows it down.\nI don’t doubt that result since some of the most authoritative information can often be found in PDFs randomly floating around the internet!\nFor anyone wondering, my coworker has a new laptop on the way regardless, since he can’t work with it constantly rebooting.\n","slug":"macos-check-shutdown-cause","tags":["crashes","logging","macos"],"title":"How can I find out why my Mac has restarted?"},{"content":"While you can delete stock folders such as Templates, Public and so on, they’ll still appear in the sidebar of your file explorer.\nThe good news is that they’re pretty easy to disable.\nReferring to the xdg-user-dirs manual shows us that there is a configuration file of “well known” user directories that lives at $HOME/.config/user-dirs.dirs by default\nSimply deleting the various entries inside might break a number of things but if you look closely, you’ll spot that changing a directory to point to your home directory will disable it\nFor example:\n\u003e cat $HOME/.config/user-dirs.dirs XDG_TEMPLATES_DIR=\"$HOME\" # templates is now disabled This should cause the Templates folder to disappear from the sidebar of Nautilus although you might need to restart first.\n","slug":"linux-disable-stock-folders","tags":["housekeeping","linux","xdg"],"title":"How can I get rid of the default application folders that ship with my Linux distro?"},{"content":"This has been something that has plagued me for years and I’ve never sat down to properly fix it.\nInstead, I’ve just added .DS_Store to .gitignore files probably over one hundred times by over.\nAnyway, the git documentation mentions the existence of a variable called core.excludesFile.\nIf you don’t set it, and $XDG_CONFIG_HOME isn’t overridden, you can add global ignores to $HOME/.config/git/ignore.\nLet’s see this in action. First we’ll make a brand new Git repository and add a .DS_Store file.\n\u003e mkdir sports \u003e cd sports \u003e git init Initialized empty Git repository in /Users/marcus/Code/sports/.git/ \u003e touch .DS_Store \u003e git status On branch main No commits yet Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) .DS_Store nothing added to commit but untracked files present (use \"git add\" to track) Ah yes, the perpetual hell but let’s try out our new trick.\n\u003e echo \".DS_Store\" \u003e\u003e ~/.config/git/ignore \u003e git status On branch main No commits yet nothing to commit (create/copy files and use \"git add\" to track) Mwah, beautiful.\n","slug":"git-globally-ignore-files","tags":["git"],"title":"How can I globally ignore files?"},{"content":"If you’ve ever seen those pesky default folders like Public and Movies, the good news is that you can get rid of them.\nYou can’t, or more specifically, you shouldn’t fully delete them as some applications may assume their existence but you can get close enough.\nLet’s say we want to hide Public, you can hide it from Finder like so:\nchflags hidden ~/Public The next time you navigate to your Home directory using Finder, you’ll see that they’ve magically disappeared\nIf you want to hide multiple at once, you can provide a comma delimited list:\nchflags hidden ~/{Downloads,Public} If, for whatever reason, you wanted to block anyone or anything from accessing those folders as well, you could use chmod to do that:\nchmod 000 ~/{Downloads,Public} Personally, I don’t bother with this step but you might have a use for it.\nThe one issue with the above is that you’ll see those files appear in your Terminal and I don’t know about you but that basically makes this whole exercise pointless.\nThere are ways to do it but I haven’t looked into them myself.\n","slug":"macos-hide-home-folders","tags":["housekeeping","macos"],"title":"How can I hide folders in my Home directory?"},{"content":"Using pg_restore, it’s almost the same process as pg_dump but in reverse\npg_restore --dbname={{DBNAME}} --host={{HOST}} --port={{PORT}} --username={{USERNAME}} --password --jobs 2 {{NAME}}.dump ","slug":"postgres-import-db","tags":["databases","postgres"],"title":"How can I import a dumped database into Postgres?"},{"content":"Until recently, I never had to go near SAML with a 10 foot pole but I was recently helping out a coworker with adding SAML authentication to an Elasticsearch cluster.\nI had never seen one before but a SAML request looks a little like this:\nhttps://idp.example.org/SAML2/SSO/Redirect?SAMLRequest=fZFfa8IwFMXfBb9DyXvaJtZ1BqsURRC2Mabbw95ivc5Am3TJrXPffmmLY3%2FA15Pzuyf33On8XJXBCaxTRmeEhTEJQBdmr%2FRbRp63K3pL5rPhYOpkVdYib%2FCon%2BC9AYfDQRB4WDvRvWWksVoY6ZQTWlbgBBZik9%2FfCR7GorYGTWFK8pu6DknnwKL%2FWEetlxmR8sBHbHJDWZqOKGdsRJM0kfQAjCUJ43KX8s78ctnIz%2Blp5xpYa4dSo1fjOKGM03i8jSeCMzGevHa2%2FBK5MNo1FdgN2JMqPLmHc0b6WTmiVbsGoTf5qv66Zq2t60x0wXZ2RKydiCJXh3CWVV1CWJgqanfl0%2Bin8xutxYOvZL18NKUqPlvZR5el%2BVhYkAgZQdsA6fWVsZXE63W2itrTQ2cVaKV2CjSSqL1v9P%2FAXv4C I took this example from Wikipedia and it’s a pretty good illustration of where the juicy part of the request probably is.\nA basic way to inspect this request in Python would look like the following. I don’t claim that this will work on all requests. For that, try something like python3-saml.\nfrom base64 import b64decode from urllib.parse import unquote import zlib url = \"fZFfa8IwFMXfBb9DyXvaJtZ1BqsURRC2Mabbw95ivc5Am3TJrXPffmmLY3%2FA15Pzuyf33On8XJXBCaxTRmeEhTEJQBdmr%2FRbRp63K3pL5rPhYOpkVdYib%2FCon%2BC9AYfDQRB4WDvRvWWksVoY6ZQTWlbgBBZik9%2FfCR7GorYGTWFK8pu6DknnwKL%2FWEetlxmR8sBHbHJDWZqOKGdsRJM0kfQAjCUJ43KX8s78ctnIz%2Blp5xpYa4dSo1fjOKGM03i8jSeCMzGevHa2%2FBK5MNo1FdgN2JMqPLmHc0b6WTmiVbsGoTf5qv66Zq2t60x0wXZ2RKydiCJXh3CWVV1CWJgqanfl0%2Bin8xutxYOvZL18NKUqPlvZR5el%2BVhYkAgZQdsA6fWVsZXE63W2itrTQ2cVaKV2CjSSqL1v9P%2FAXv4C\" urldecoded_url = unquote(url) b64decoded_url = b64decode(url) request = zlib.decompress(b64decoded_url, -15).decode() print(request) // '\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\\r\\n\u003csamlp:AuthnRequest\\r\\n xmlns:samlp=\"urn:oasis:names:tc:SAML:2.0:protocol\"\\r\\n xmlns:saml=\"urn:oasis:names:tc:SAML:2.0:assertion\"\\r\\n ID=\"aaf23196-1773-2113-474a-fe114412ab72\"\\r\\n Version=\"2.0\"\\r\\n IssueInstant=\"2004-12-05T09:21:59Z\"\\r\\n AssertionConsumerServiceIndex=\"0\"\\r\\n AttributeConsumingServiceIndex=\"0\"\u003e\\r\\n \u003csaml:Issuer\u003ehttps://sp.example.com/SAML2\u003c/saml:Issuer\u003e\\r\\n \u003csamlp:NameIDPolicy\\r\\n AllowCreate=\"true\"\\r\\n Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:transient\"/\u003e\\r\\n\u003c/samlp:AuthnRequest\u003e\\r\\n' If you’re feeling a bit lazy, like I often am, you can use any of the online decoders, such as this one by PingID.\nIf you’re dealing with sensitive credentials however, it’s best to decode it locally rather than trusting a third party.\n","slug":"saml-inspect-request","tags":["authentication","saml"],"title":"How can I inspect a SAML request?"},{"content":"While you could provide a button, some parts of a site can look quite nice if they automatically transition between light and dark mode.\nYou can listen for these changes like so:\nwindow.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e =\u003e { const updatedScheme = e.matches ? \"dark\" : \"light\" if (updatedScheme === \"dark\") { // change something to dark mode } else { // change something to light mode } }) ","slug":"js-colour-scheme-listener","tags":["darkmode","javascript"],"title":"How can I listen for user changes to their colour scheme (ie dark mode)?"},{"content":"DNS! It’s always the answer for your woes :)\nWhile there are a myriad of HTTP servers for seeing your external IP address, you can also use one of the various DNS based services on offer.\nThese will give you an IPv4 flag. The -4 flag isn’t necessarily required but without explicitly providing it, you’ll be gambling on the return type.\n\u003e dig @resolver3.opendns.com myip.opendns.com +short -4 \u003e dig @resolver4.opendns.com myip.opendns.com +short -4 \u003e dig @ns1-1.akamaitech.net ANY whoami.akamai.net +short -4 \u003e dig @ns1.google.com TXT o-o.myaddr.l.google.com +short -4 and likewise, for IPv6\n\u003e dig @resolver1.ipv6-sandbox.opendns.com AAAA myip.opendns.com +short -6 \u003e dig @ns1.google.com TXT o-o.myaddr.l.google.com +short -6 You can read more, and see some other providers I left out, in this detailed StackOverflow thread but generally speaking, I’ve found OpenDNS’s resolver4 to be the fastest of the lot on offer.\nA very handy thing to have aliased and way quicker than clicking 5 times to navigate to a webpage.\n","slug":"dns-lookup-current-ip","tags":["dig","dns"],"title":"How can I look up my current external IP address?"},{"content":"For large downloads, such as macOS updates, it can be annoying that tools like Self Service don’t surface download metrics\nThankfully, we can find the download on disk and watch as the file size increases\nIn the case of macOS, downloads live at /Library/Application\\ Support/JAMF/Downloads\nI’m no shell scripting master but the following is a quick hack to view the progress in real time\nThere are better tools like watch but eh, this works fine enough\nHere’s the script I’ve been using but it requires gnumfmt which you can install with brew install coreutils\n\u003e while (true) do echo $(sudo ls -l /Library/Application\\ Support/JAMF/Downloads | grep macOS | awk '{ print $5 }' | gnumfmt --to iec --format \"Downloaded: %8.1f\"); sleep 15; done Downloaded: 6.9G Downloaded: 7.0G Downloaded: 7.0G Downloaded: 7.1G Downloaded: 7.1G That’s not particulary readable so here’s a bit of an explainer:\nwhile (true) do echo $( sudo ls -l /Library/Application\\ Support/JAMF/Downloads | # (1) grep macOS | # (2) awk '{ print $5 }' | # (3) gnumfmt --to iec --format \"Downloaded: %8.1f\" # (4) ); sleep 15; # (5) done Annoyingly, JAMF/Downloads is a restricted directory so we have to be a superuser in order to operate within that folder We’re only concerned with one column in particular, in my case it’s the macOS Big Sur DMG Let’s fetch the current file size but just seeing 8466481152 is not particularly useful We can use gnumfmt, a GNU utils implementation of numfmt given the latter only exists on Linux systems. gnumfmt is available via Homebrew as mentioned above We just run this script continually until Ctrl-C is invoked. Over a average speed proxy, it takes about 45 seconds to download 100MB so there’s no value personally in setting something like sleep 5 Enjoy your window into frustration as you realise just how long waiting will take\n","slug":"macos-monitor-jamf-downloads","tags":["enterprise","jamf","macos","software"],"title":"How can I monitor JAMF downloads on macOS?"},{"content":"If you have a cronjob that you’d like to pause while doing some maintenance for example, you can use the suspend attribute.\n\u003e kubectl patch cronjobs does-something -p '{\"spec\": {\"suspend\": true }}' cronjob.batch/does-something patched Once you’re done, you can just flip true to false\n","slug":"kubes-pause-recurring-cronjob","tags":["cronjob","kubernetes"],"title":"How can I pause a recurring Kube cronjob?"},{"content":"How can I perform a regex search? You can check if a character contains a string by using the cmatch operator like so:\n$word = \"Hello\" $word -cmatch \"[A-Z]\" // True ","slug":"powershell-regex","tags":["powershell"],"title":"How can I perform a regex search in Powershell?"},{"content":"Often times, it can be useful to check the value of a Kubernetes secret, to check that it lines up with what an application is receiving. An example might be a randomly generated secret that is shared between multiple Kubernetes resources.\nLet’s have a look at a mock secret:\n\u003e kubectl describe secret dummy-secret Name: dummy-secret Namespace: sports Labels: app.kubernetes.io/managed-by=Helm Annotations: meta.helm.sh/release-name: sports meta.helm.sh/release-namespace: sports Type: Opaque Data ==== MY_FAVOURITE_FRUIT: 12 bytes So here we have a secret called dummy-secret and one of the values within it has the name MY_FAVOURITE_FRUIT.\nWe can fetch it like so:\n\u003e kubectl get secret dummy-secret -o jsonpath=\"{.data.MY_FAVOURITE_FRUIT}\" | base64 --decode strawberries ","slug":"kubes-read-secret","tags":["credentials","kubernetes","security"],"title":"How can I read a Kubernetes secret?"},{"content":"I recently came across an unsecured Selenium instance but I wanted to confirm my findings by making a basic request.\nWhile I opted to use the Python bindings for Selenium, there wasn’t a quick start guide on how to remotely connect to an instance.\nHere’s how you can quickly connect to a Selenium instance and do a basic request using Python:\n\u003e\u003e\u003e from selenium.webdriver.common.desired_capabilities import DesiredCapabilities \u003e\u003e\u003e from selenium import webdriver \u003e\u003e\u003e hub_url = \"http://example.com:4444/wd/hub\" \u003e\u003e\u003e driver = webdriver.Remote(command_executor=hub_url, desired_capabilities=DesiredCapabilities.CHROME) \u003e\u003e\u003e driver.get(\"https://news.ycombinator.com\") \u003e\u003e\u003e driver.find_element_by_tag_name(\"img\").get_attribute(\"src\") 'https://news.ycombinator.com/y18.svg' ","slug":"selenium-remote-connection","tags":["python","selenium"],"title":"How can I remotely connect to a Selenium cluster"},{"content":"This one had my scratching my head a bit as I wasn’t quite sure if Kubernetes was the right place to do this.\nDepending on your use case, it might make sense to terminate traffic before it reaches your cluster but that may have the effect of filtering traffic to other applications if not done properly.\nIn this instance, the Kubernetes cluster in question makes use of the NGINX Ingress Controller and as such, honours a whole bunch of flags.\nBefore we get into the details, let’s set up a small example.\nWe’ll pretend our desktop has an IP address is 192.0.2.3 exactly. We want to allow only a network range of 1 single address so that say; our mobile device with the address 192.0.2.2 can’t connect but our desktop can.\nIn CIDR notation, this would be represented as 192.0.2.3/32, with the 32 effectively meaning “Just this one address” instead of any other devices on the 192.0.2 range, or broader.\nWith our address block defined, let’s look at an ingress:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-cool-ingress annotations: nginx.ingress.kubernetes.io/whitelist-source-range: \"192.0.2.3/24\" spec: rules: - host: example.com http: paths: - path: / backend: service: name: example-docs port: name: http-example-docs Ok, we’ve allowed our desktop to connect but let’s try connecting to this ingress from a device we know isn’t allowed, such as our laptop on 192.0.2.6:\n\u003e curl https://example.com/ \u003chtml\u003e \u003chead\u003e\u003ctitle\u003e403 Forbidden\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003ccenter\u003e\u003ch1\u003e403 Forbidden\u003c/h1\u003e\u003c/center\u003e \u003chr\u003e\u003ccenter\u003enginx\u003c/center\u003e \u003c/body\u003e \u003c/html\u003e Alright, and now from our desktop at 192.0.2.3/24, which we allowed explicitly:\n\u003e curl example.com \u003c!doctype html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eExample Domain\u003c/title\u003e [...] Success! We’ve managed to use nothing but an ingress to block specific traffic but you might wonder, why would I ever use this?\nOne use case may be exposing applications that require the use of a public endpoint, such as Microsoft Teams or Slack.\nOften, you can’t make use of OAuth but you want to protect against random internet traffic so you can explicitly allow known IP ranges.\nWith Azure for example, they publish a full list of their active IP ranges so if you can’t simply make use of a VNet, this may be the next best thing.\n","slug":"kubes-ingress-ip-range","tags":["allowlist","kubernetes"],"title":"How can I restrict which traffic is allowed to pass through a Kube ingress?"},{"content":"This issue is particularly annoying and I only just discovered it today for the first time.\nHere’s an example of what it looks like\nIn order to install the application so that it bypasses Gatekeeper, you can rerun brew cask install like so:\n\u003e brew cask install --no-quarantine blah \u003e brew reinstall --no-quarantine blah If you’d like to keep this flag enabled all the time, and honestly you might as well, you can also do the following:\n\u003e export HOMEBREW_CASK_OPTS=\"--no-quarantine\" \u003e brew cask install blah ","slug":"macos-homebrew-app-blocked","tags":["gatekeeper","homebrew","macos"],"title":"How can I run a Homebrew application being blocked by Gatekeeper?"},{"content":"For those of us who are subject to using corporate VPNs, all sorts of wackiness can occur such as 127.0.0.1 being routed first to another country before trying to resolve locally.\nYou can see both IPv4 and IPv6 routing entries by running netstat -rn. Personally, I like to just show IPv4 addresses.\nHere’s an example of my route table with WiFi (and ethernet) interfaces disabled:\n\u003e netstat -nr -f inet Routing tables Internet: Destination Gateway Flags Netif Expire 127 127.0.0.1 UCS lo0 127.0.0.1 127.0.0.1 UH lo0 111.0.0 link#1 UmCS lo0 I’ve changed the last entry since I don’t actually know if it’s an internet work address.\n","slug":"macos-view-route-table","tags":["macos","networking","vpn"],"title":"How can I see my route table?"},{"content":"Recently I had noticed that some shell commands on my laptop were executing surprisingly slow.\nLike most things in the tech world, it was due to a piece of jamf software locking up anything that was being read.\nI managed to validate this assumption using the command fs_usage which requires sudo. Here’s an example of it in action.\n\u003e sudo fs_usage | grep zshrc Password: 16:19:22 open /Users/marcus/dotfiles/zsh/zshrc.md 0.000021 lugh 16:19:22 open /Users/marcus/dotfiles/zsh/.zshrc 0.000137 lugh 16:19:22 WrData[A] /Users/marcus/dotfiles/zsh/.zshrc 0.000324 W lugh 16:19:22 lstat64 /System/Volumes/Data/Users/marcus/dotfiles/zsh/.zshrc 0.000015 fseventsd 16:19:22 lstat64 dotfiles/zsh/.zshrc 0.000005 perl5.28 16:19:22 lstat64 .zshrc 0.000007 perl5.28 16:19:22 lstat64 .zshrc 0.000004 perl5.28 16:19:22 readlink .zshrc 0.000004 perl5.28 16:19:22 stat64 dotfiles/zsh/.zshrc/.stow 0.000002 perl5.28 16:19:22 stat64 dotfiles/zsh/.zshrc/.nonstow 0.000001 perl5.28 16:19:22 stat64 dotfiles/zsh/.zshrc 0.000004 perl5.28 16:19:22 fsgetpath /Users/marcus/dotfiles/zsh/.zshrc 0.000005 Finder 16:19:22 getattrlist /Users/marcus/dotfiles/zsh/.zshrc 0.000014 Finder 16:19:22 fsgetpath /Users/marcus/dotfiles/zsh/.zshrc 0.000005 Finder 16:19:22 fsgetpath /Users/marcus/dotfiles/zsh/zshrc.md 0.000005 Finder 16:19:22 getattrlist /Users/marcus/dotfiles/zsh/zshrc.md 0.000012 Finder 16:19:22 fsgetpath /Users/marcus/.zshrc 0.000005 Finder 16:19:22 getattrlist /Users/marcus/.zshrc 0.000015 Finder 16:19:22 fsgetpath /Users/marcus/zshrc.md 0.000005 Finder 16:19:22 getattrlist /Users/marcus/zshrc.md 0.000014 Finder 16:19:22 fsgetpath /Users/marcus/zshrc.md 0.000003 Finder 16:19:22 getxattr dotfiles/zsh/zshrc.md 0.000014 Finder 16:19:22 fsgetpath /Users/marcus/zshrc.md 0.000004 Finder 16:19:22 fsgetpath /Users/marcus/zshrc.md 0.000003 Finder 16:19:23 lstat64 /System/Volumes/Data/Users/marcus/dotfiles/zsh/.zshrc 0.000005 fseventsd Now this output doesn’t actually come from my work computer so you won’t see the mentioned JamfAgent but we can walk through this anyway.\nFirst is lugh, a custom and possibly temporary literate markdown tool I use on my dotfiles. Next is perl, in the form of GNU Stow followed by macOS Finder doing some things. This gives a really nice breakdown of what is going on.\nYou can even use it to better understand applications, like if you run git status and see all the files that were touched within the .git folder.\nI actually spotted that Yet Another Daemon was touching some of my .git files on my work laptop too. Shoo!\n","slug":"macos-see-file-usage","tags":["enterprise","jamf","macos","performance","terminal"],"title":"How can I see what applications are making my shell commands slow?"},{"content":"By default, kubectl will search the default namespace for any newly added clusters to your context, which can be quite annoying.\nYou can of course tack on -n \u003cnamespace\u003e manually or make your own little wrapper around kubectl as I have.\nA simpler version though is to just do the following:\nkubectl config set-context --current --namespace=baseball Context \"sports\" modified Where baseball is the name of your namespace of course.\nGoing forward, any commands will default to use the baseball namespace but you can override them as always with -n.\n","slug":"kubes-default-namespace","tags":["defaults","kubectl","kubernetes"],"title":"How can I set a default kubectl namespace for a given cluster?"},{"content":"Often times, you might want to test connectivity to a container but without doing so from within the container itself. You could just into a neighbouring pod but it may not have networking tools (ie tools) or even potentially network connectively if there’s a network policy in the mix.\nA quick way to deploy a curl container has been shared before in the Kubernetes docs and it looks like this:\n\u003e kubectl run curl --image=radial/busyboxplus:curl -i --tty Unable to use a TTY - container curl did not allocate one If you don't see a command prompt, try pressing enter. I think the output looks something like that but this is a bit more involved as my work makes use of policies in our cluster.\nNow normally I just keep a file called curl-debug.yml sitting around my hard drive and deploy it using kubectl apply -f curl-debug.yml but you can also deploy it inline using a hideously log container override.\nYou may need more (or less) override fields depending on eg; if your network policy only allows pods with certain annotations or metadata to connect to what you’re testing.\nAn unprivileged curl pod would look something like this. Note that I’ve removed -i --rm --tty as it always seems buggy to me and I much prefer to just manually run kubectl exec -it curl -- sh than have my terminal hanging.\n\u003e kubectl run curl --image=radial/busyboxplus:curl --overrides='{ \"spec\": { \"securityContext\": { \"runAsUser\": 1000, \"runAsGroup\": 1000, \"seccompProfile\": { \"type\": \"RuntimeDefault\" }}, \"containers\": [{ \"name\": \"curl\", \"image\": \"radial/busyboxplus:curl\", \"command\": [ \"/bin/sh\", \"-c\", \"--\" ], \"args\": [ \"while true; do sleep 30; done; \" ], \"securityContext\": { \"runAsNonRoot\": true, \"allowPrivilegeEscalation\": false }}]}} pod/curl created and for those who don’t love huge eyesores, here’s the contents of the pod spec I alluded to earlier:\napiVersion: v1 kind: Pod metadata: name: \"curl\" labels: app: \"my-cool-app\" service: \"some-other-identifier\" spec: securityContext: runAsUser: 1000 runAsGroup: 1000 seccompProfile: type: RuntimeDefault containers: - name: \"curl\" image: \"radial/busyboxplus:curl\" command: [ \"/bin/sh\", \"-c\", \"--\" ] args: [ \"while true; do sleep 30; done;\" ] securityContext: runAsNonRoot: true allowPrivilegeEscalation: false Ah right, the actual point of the question. Once you have curl running, and you’re inside the container, you can then use curl to test out the connectivity of things.\nFor example, earlier today I was moving a container to a new cluster and it was using the URL that the ingress was listening to. Let’s use https://sports.example.com in this case and say that the service was called sports.\nThe ingress URL changed from being internally accessible to publically accessable, although behind an OAuth2 proxy of course.\nI noticed this change by doing the following:\n\u003e curl --head --location http://sports HTTP/1.1 200 OK Server: nginx [...] Ok, it resolves the internal service perfectly fine. How about the public one?\n\u003e curl --head --location https://sports.example.com HTTP/1.1 302 Moved Temporarily Location: https://example.com/_oauth2/start?rd=https://sports.example.com [...] HTTP/1.1 302 Found Location: https://login.microsoftonline.com/common/oauth2/authorize?a_very_long_string [...] HTTP/1.1 200 OK Content-Length: 186288 [...] Now, I don’t even have to look at the payload to infer that we probably just hit an OAuth2 login page and that’s exactly what was happening.\nIn the previous cluster, we were using internal links to the external OAuth proxy is never involved. Admittedly, this was rationalised as “DNS just magically knows to resolve the request to the service right next door” and perhaps this is true but maybe not!\nAnyway, a third case that you might run into is the following:\n\u003e curl --head --location http://something # the command just sits with no output forever! If you check my curl-debug.yaml, I have specific labels that the network policy looks for. Because this curl pod is missing them, it can’t make any requests.\nThis could be anything from protocol (TCP/UDP), port number, whitelisted namespaces, whitelisted resources and so on. If you have this problem, either check your various log messages for reference to a required policy or check for an existing one that needs to be updated.\nDoing a search for “kind: NetworkPolicy” should help narrow down which files are relevant and/or if they even exist in the first place.\nHappy debugging!\n","slug":"kubes-namespace-connectivity","tags":["curl","debugging","kubernetes","networking"],"title":"How can I test connectivity within my Kube namespace?"},{"content":"There is a CLI tool called xcall which seems to be the only way I’ve seen to actually interact with x-callback-url outside of other applications.\nIt’s a bit wonky in that you have to drag xcall.app to your Applications folder and then either add that to your path or reference the cli tool inside directly.\nHere’s an example of it in use:\n\u003e /Applications/xcall.app/Contents/MacOS/xcall -url \"things:///version\" -activateApp NO { \"x-things-client-version\" : \"31310506\", \"x-things-scheme-version\" : \"2\" } Annoyingly, this will activate the application in question, if it isn’t already open, but that is the nature of x-callback-url after all.\nIt will take the foreground view upon opening but further invocations won’t trigger it, assuming you use -activateApp NO. If you want it to appear, such as when triggering a search, you can use -activateApp YES instead.\n","slug":"macos-invoke-x-callback-url","tags":["macos","x-callback-url"],"title":"How can I try out x-callback-url commands on macOS?"},{"content":"From time to time, I have troubles with Firefox since it seems to clash with a corporate proxy we use.\nUsing the built-in certificate store rather than Firefox’s own managed store seemed to “fix” this issue.\nTo do this, you’ll want to navigate to about:config and then toggle security.enterprise_roots.enabled to true.\n","slug":"firefox-local-cert-store","tags":["browsers","enterprise","firefox","software"],"title":"How can I use my local certificate store with Firefox?"},{"content":"Let’s assume you have multiple networks set up under System Preferences \u003e Networks.\nYou might have “Work” which has a bunch of proxy configuration specified and “Home” which just disabled proxy configuration.\nIf you left the former “Work” network selected, then went to a place that can’t access the proxy server, you wouldn’t be able to access the internet and vice versa.\nTo make automating this a little bit easier, there’s a command line tool called scselect\nHere’s an example of what it looks like in action:\n\u003e scselect Defined sets include: (* == current set) * \u003cguid\u003e (Work) \u003cguid\u003e (Home) In this example, we can see the Work network is selected.\nNow we wanted to change to the Home network, you could do so manually in System Preferences or you can run scselect with the name of the network you want to change to like so:\n\u003e scselect Home CurrentSet updated to \u003cguid\u003e (Home) \u003e scselect Defined sets include: (* == current set) \u003cguid\u003e (Work) * \u003cguid\u003e (Home) As you can see, the Home network is now selected.\nI only recently discovered this tool so I haven’t automated it yet but it’s probably feasible to have a file with your working hours and then if it’s within those hours, toggle on the Work network (and all of the proxy configuration that comes with it)\nThe reason you might want to use a schedule and not eg; WiFi name is that you might be working from home over a VPN for example.\n","slug":"macos-configured-networks","tags":["macos","networking"],"title":"How can I view configured networks in my macOS terminal?"},{"content":"The iex interpreter includes a function called h which can be used to show documentation for a module\nh String # h/1 ","slug":"elixir-help-docs","tags":["elixir"],"title":"How can I view help documentation for an Elixir module?"},{"content":"Let’s say we have the following module\ndefmodule Reminder do def alarm(time, day) do end end We can check what methods are on it by providing a :functions atom\nReminder.__info__(:functions) # [alarm: 2] As we can see, this Reminder module has an alarm method, with an arity of 2.\n","slug":"elixir-object-methods","tags":["elixir"],"title":"How can I view methods associated with an Elixir object?"},{"content":"Let’s say that you have a variable that contains a string:\n$a = \"abc\" That’s neat but what if I want to view the possible methods that are available on the string object? You can use the Get-Member cmdlet. You can also use the shorthand version gm.\n$a = \"abc\" $a | gm // Name | MemberType | Definition // Clone | Method | System.Object Clone() [...] // ... // Length | Property | int Length {get;} $a.Length // 3 You can also view the static methods associated with an object too:\n\"abc\" | gm - Static // Compare | Method | static int Compare(string strA, string strB)... ","slug":"powershell-object-methods","tags":["powershell"],"title":"How can I view the methods associated with an object?"},{"content":"According to the Storage part of the Prometheus documentation, a single sample is somewhere between 1 - 2 bytes.\nYou can roughly calculate how much storage you’ll need with the following formulae:\ndisk_space = retention_time_in_seconds * samples_ingested_per_second * 2 bytes (take the upper to be safe) By that logic, if we were ingesting 4000 samples per second and we were retaining them for 15 days (the default), it would look something like this:\ndisk_space = 1296000 * 4000 * 2 disk_space // 10368000000 bytes disk_space in gigabytes // 10.37 gigabytes Given this, you can see the levers you have are decreasing the amount of sampling going on, reducing the amount of time samples are retained for or simply buying more disk space as you go on.\nRemember as well that we took the high end of the estimation and it could be as low as 5.185 if we’re extremely lucky on compression and/or presumably we have next to no labels on each sample.\nYou would also need to factor in many other things such as the write-ahead log but I don’t pretend to know what any of these things are.\nI just use Prometheus! I don’t actually maintain a cluster or anything like that.\n","slug":"prometheus-sample-size","tags":["monitoring","prometheus","timeseries"],"title":"How large is a single Prometheus sample?"},{"content":"Back in the day, there was just one file: HOSTS.TXT.\nIt contained a name-to-address mapping for every entity within ARPANET.\n/etc/hosts used to be compiled from HOSTS.TXT\nIt didn’t scale for a number of reasons:\nAs soon as administrators pulled the latest version of HOSTS.TXT, it would already be out of date There was no way to enforce constraints eg; no duplicates on hostnames It took a lot of resources to serve it up to every administrator ","slug":"dns-original-implementation","tags":["dns","historical"],"title":"How was DNS originally implemented?"},{"content":"An initial thought might be that it would help to capture all context about everything, all of the time but that would soon get very expensive to store.\nProfiling takes the approach of capturing as much context as possible for a certain period of time, generally for use in debugging.\nContinually gathering information, such as how long each function took to execute, in a production environment would very quickly impact end users so this is best suited for validating targeted assumptions of what might be going wrong.\n","slug":"monitoring-what-is-profiling","tags":["instrumentation","monitoring"],"title":"What is profiling?"},{"content":"The root node of DNS has a null label\nThe DNS tree is restricted to 127 levels of depth so you could only.have.a.domain.name.one.hundred.and.twenty.seven.levels.deep.com\n. is used to mark a domain as absolute eg; utf9k.net.\nBehind the scenes, a full domain name would be www.google.com.\u003croot/null\u003e\nSome websites, or perhaps more accurately the load balancers and proxies in front of them, don’t acknowledge the existence of such a thing.\nOne high profile example is Amazon. If you visit https://amazon.com., you’ll see a blank page with the title x. Note the period on the end of the URL to see this issue in effect.\n","slug":"dns-trailing-period","tags":["dns","historical","networking"],"title":"What is the period you sometimes see at the end of a domain name?"},{"content":"In the same vein that it’s not often feasible to capture all data, all of the time, tracing is concerned with sampling a subset of events such as every 50th incoming request.\nGenerally most tracing implementations will show you how much time is spent at each step of the way from establishing an SSL connection through to how long is spent talking with any given database.\nDistributed tracing is this same idea but… well, distributed.\nMore specifically, interactions are “tagged”, whether it be an HTTP header or an attribute within an RPC call. While those interactions may pass the boundaries of any one service, they can be “stitched” back together by matching up the associated request IDs.\nThe idea here being that you can trace a request through a system oriented around microservices, as if it were just one regular application.\nGiven that only a subset of interactions (ie 1 in 100) are sampled, this solves the storage issues presented by full on profiling all of the time.\n","slug":"monitoring-tracing-overview","tags":["instrumentation","monitoring"],"title":"What is tracing?"},{"content":"I’ve been fiddling a bit with Wails recently and I gave the unreleased v2 alpha a try.\nOut of the box, it binds to Port 5000 and I was surprised to receive a 403 Forbidden.\nDefinitely not what I expected.\nWe can use the lsof utility to figure out what’s holding on to Port 5000. You’ll see in the screenshot below that I use a shell function called whomport but under the hood, it’s running lsof -nP i4TCP:5000 | grep LISTEN. Let’s see what the output looks like.\nThis doesn’t really help us much since Control Centre could be anything but a bit of searching brings up that this change was introduced in macOS Monterey.\nIn particular, there’s a new setting under System Preferences -\u003e Sharing called Airplay Receiver. Let’s toggle it off.\nOnce this is done, you should find Port 5000 instantly freed up. It’s weird that Apple would pick such a commonly used port, especially for developers!\n","slug":"macos-port-5000-monterey","tags":["airplay","macos","monterey","receiver"],"title":"What is using Port 5000 on macOS Monterey?"},{"content":"Services and libraries have different needs. Further, not all services are alike in the types of work they perform or what types of work are important to measure\nOnline-serving systems These are services that have a person or client waiting for a response.\nAs such, the RED method captures key metrics which are Requests, Errors and Duration.\nIt’s worth noting that there may be a tendency to exclude failed requsts when capturing duration but this temptation should be avoided.\nIn the event that you only had successes, a long running request that ultimate failed after 15 seconds would be excluded for example, despite any reasonable initial assumption that errors may tend towards having a lower duration.\nOffline-serving systems These are services that operate continually in the background. Their workloads are generally in batches and may utilise multiple steps, buffered with a queuing system.\nThe USE method captures key metrics which are Utilisation, Saturation and Errors.\nBatch jobs Similar to offline-serving systems, these may be kicked off upon request (ie sending an email in the background) or something akin to a cronjob.\nGiven that they aren’t suitable for serving a persistent HTTP endpoint for scraping, it’s best to push metrics to a monitoring solution such as Prometheus upon work being completed.\n","slug":"monitoring-what-to-instrument","tags":["instrumentation","monitoring"],"title":"What is worth instrumenting?"},{"content":"One of the primary considerations of the HTTP2 Working Group was definitely that encouraging HTTPS meant a more secure web.\nMore practically however, there had been previous experiments using WebSockets and SPDY which showed that regular HTTP requests were highly prone to failure due to things like proxies interrupting negotiation.\nOften times, an Upgrade header was supplied with the initial HTTP negotiation and then shortly both sides upgraded to HTTPS but if HTTPS was used from the outside, there would be a significantly easier time doing protocol negotiation.\nThere is an overhead to establishing a TLS connection of course but the price pays off in the form of HTTP2 multiplexing and so on.\n","slug":"http-non-encryption-benefits","tags":["historical","http","reliability"],"title":"What non-encryption benefits are provided by HTTPS?"},{"content":"Rob Pike explained his understanding in a now-dead Google+ post back in 2012. The use of . and .. had appeared in early versions of the Unix file system, as a quick way to navigate around. They would appear when using ls to view the contents of a directory so a line was added that ignored anything where the first character was a period.\nThis, of course, meant that any files starting with a . were also hidden and so began years of bad practices. Rather than think “Where should I store my configuration folder”, the easy option became storing a dotfile instead. It may be messy but if no one can see it, is it really so bad?\nRob also points out that configuration could just as easily be stored in $HOME/cfg or $HOME/lib as was the case in Plan 9. He doesn’t dispute that dotfiles have their uses but emphasizes that the file itself serves the purpose. Prepending a dot does not a configuration file make.\n","slug":"linux-why-do-dotfiles-exist","tags":["historical","linux","macos"],"title":"Why are dot files a thing?"},{"content":"On April 20th 2020, oil futures fell to $-37.63 per barrel but how is that possible? That would suggest people are literally paying customers to take oil off their hands.\nIn a sense, that’s exactly the case but perhaps not for quite the reasons you might expect.\nWhat are oil futures? with the pandemic bringing the economy to a standstill, there is so much unused oil sloshing around that American energy companies have run out of room to store it. And if there’s no place to put the oil, no one wants a crude contract that is about to come due.\nIn reality, traders hold a contract that, generally after about 3 months, is translated into a physical delivery of oil.\nFor example, someone may pay $40 per barrel in January. Oil may be worth $60 in March so a trader might then sell the contract for a $20 profit, as I understand it anyway.\nFutures are designed for people who actually want to purchase oil, or any future-able item like wheat, corn and so on.\nThat doesn’t stop a whole portion of Wall Street who speculate on these futures however.\nThere are some funny stories about junior traders who have forgotten to sell their contract and have been required to take delivery of hundreds of physical barrels of oil.\nReceiving delivery The city of Cushing, Oklahoma is called the “Pipeline Crossroads of the World” and is where a great deal of oil is stored in the United States.\nWhen it comes to oil futures, Cushing is also the one and only designated delivery point.\nNormally, oil is stored there on behalf of those who lease between 50 - 80 million barrels worth of storage.\nWhy did oil futures turn negative? Given Coronavirus meant that most of the world was at home, oil producers had little option but to store their oil.\nWhen it came time for those futures to convert into physical oil, there was no actual storage left for traders to purchase.\nWith nowhere to store their oil, and the foreboding promise of exorbitant storage fees at the delivery site, the holders of oil delivery contracts were forced to pay buyers with oil storage contracts to take the product off their hands, upending the market and sending the entire industry into unknown, negative territory.\nFaced with the horrifying idea of having to drive all the way to Cushing, Oklahoma to somehow receive thousands of barrels of oil, it became more appeals to just pay almost $40/barrel for someone else to take the future (and impending delivery) off of their hands\nStorage may have been possible but at a time where it was in high demand, the excesses would have been astronomical.\nNot to mention, there was already an overabundance of oil for the foreseeable future so it would take a long time to realise a profit, if at all.\nSources https://www.bloomberg.com/opinion/articles/2020-04-22/nobody-wants-much-oil-right-now\nhttps://www.bloomberg.com/opinion/articles/2020-04-28/oil-traders-not-sure-they-like-oil\nhttps://www.reuters.com/article/us-global-oil-usa-storage/no-vacancy-main-us-oil-storage-in-cushing-is-all-booked-idUSKCN22332W\nhttps://www.cushingcitizen.com/news/oil-turns-red\nFurther reading https://www.npr.org/sections/money/2016/08/26/491342091/planet-money-buys-oil ","slug":"finance-oil-futures-negative","tags":["finance","futures"],"title":"Why did oil futures go negative in April 2020?"},{"content":"It was common to have images on a subdomain and the bulk of the site at the root of a domain such as nytimes.com and img.nytimes.com\nCaching is widely understood as the current value but it doesn’t capture the historical context behind the introduction of this tactic.\nAnother aspect is that the size of headers bloated significantly, sometimes to where cookies associated with a request would be larger than a single TCP packet, which is about 1.5kb.\nIn order to reduce latency, it made sense to move resources that didn’t require cookies to a separate domain, so that those requests didn’t inherit excess headers. While not large on a single request, requests for multiple assets would balloon exponentially.\nThis practice was colloquially referred to as a “cookie-less domain”.\n","slug":"http-domain-splits","tags":["cookies","headers","historical","http"],"title":"Why did sites split their assets across multiple domains back in the day?"},{"content":"The Playstation 1 uses CD-ROMs with the XA extension.\nIf you open a PS1 ISO within a hex editor, you’ll want to scroll down to offset 37656\nWithin a normal CD (or ISO), it will have a header offset size of 2048.\nIt seems a little arbitrary but you can read the table of contents by multiplying the header offset by 16.\nFor a normal CD, this would be 2048 * 16 = 32768\nIn the case of PS1 discs, the header offset size is 2352 for reasons I don’t understand so they start at 2352 * 16 = 37632\nLastly, and somewhat arbitrarily, you’ll want to jump forward by an additional 24 bytes in order to come to the starting point of 37565 points\n","slug":"ps1-disc-offset","tags":["hexedit","playstation","videogames"],"title":"Why do Playstation 1 discs start at offset 37656?"},{"content":"As an example of what I mean, org-roam had seemingly the same function names at one point, despite the only difference being some double dashes\nHere is an example\nAt first glance, the naming differences between org-roam-capture--get-point and org-roam--capture-get-point seems completely arbitrary\nSupposedly, since there is no such thing as internal vs external functions, it’s a convention for declaring that a function should be considered private or internal only\nI still don’t understand the above example since they both have double hyphens\n","slug":"emacs-function-double-dash","tags":["elisp","emacs"],"title":"Why do some Emacs functions have double dashes?"},{"content":"Lists that start with a ` end up having values interpolated.\nCompare the following two examples:\n'(,(concat \"Hello, \" \"World\"), \"Nice to meet you?\") ; (,(concat \"Hello, \" \"World\") ; ,\"Nice to meet you?\") As you can see, we got the exact same list that we defined when starting with a '\nHow about using a `?\n`(,(concat \"Hello, \" \"World\"), \"Nice to meet you?\") ; (\"Hello, World\" \"Nice to meet you?\") The concat expression is evaluated and we get back two strings!\n","slug":"emacs-list-backtick","tags":["elisp","emacs"],"title":"Why do some Emacs lists start with a backtick instead of a comma?"},{"content":"I recently ran into this issue when switching my distro to Manjaro.\nI’d find that whenever a different audio source would start playing such as a voice call, notification or even a silent video on the web, my Spotify audio would drop to 0 instantly\nIn order to fix this, all I needed to do was unload the module-role-cork module presumably used by pulseaudio\nYou can toggle via your terminal to test that it works like so:\n\u003e pactl unload-module module-role-cork # disabled, try spotify and another audio source \u003e pactl load-module module-role-cork # enabled, spotify should be interrupted While I’m not sure how long unload-module persists (I’m guessing until the next restart), you can achieve the same effect by commenting out the module in the configuration for pulseaudio like so:\n\u003e grep \"cork\" /etc/pulse/default.pa -B 3 ### Cork music/video streams when a phone stream is active # Disabling this allows audio streams to run over the top of each other # Before this, a newer stream (notification, video) would mute Spotify #load-module module-role-cork Once that’s done, you should be good to go. It seems to work as expected for me anyway.\n","slug":"linux-audio-muting-suddenly","tags":["audio","bugs","linux","spotify"],"title":"Why do some of my applications suddenly get muted on Linux?"}]